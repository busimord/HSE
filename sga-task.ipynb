{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User routes on the site\n",
    "## Description\n",
    "**Clickstream** is a sequence of user actions on a website. It allows you to understand how users interact with the site. In this task, you need to find the most frequent custom routes.\n",
    "\n",
    "## Input data\n",
    "Input data is а table with clickstream data in file `hdfs:/data/clickstream.csv`.\n",
    "\n",
    "### Table structure\n",
    "* `user_id (int)` - Unique user identifier.\n",
    "* `session_id (int)` - Unique identifier for the user session. The user's session lasts until the identifier changes.\n",
    "* `event_type (string)` - Event type from the list:\n",
    "    * **page** - visit to the page\n",
    "    * **event** - any action on the page\n",
    "    * <b>&lt;custom&gt;</b> - string with any other type\n",
    "* `event_type (string)` - Page on the site.\n",
    "* `timestamp (int)` - Unix-timestamp of action.\n",
    "\n",
    "### Browser errors\n",
    "Errors can sometimes occur in the user's browser - after such an error appears, we can no longer trust the data of this session and all the following lines after the error or at the same time with it are considered corrupted and **should not be counted** in statistics.\n",
    "\n",
    "When an error occurs on the page, a random string containing the word **error** will be written to the `event_type` field.\n",
    "\n",
    "### Sample of user session\n",
    "<pre>\n",
    "+-------+----------+------------+----------+----------+\n",
    "|user_id|session_id|  event_type|event_page| timestamp|\n",
    "+-------+----------+------------+----------+----------+\n",
    "|    562|       507|        page|      main|1620494781|\n",
    "|    562|       507|       event|      main|1620494788|\n",
    "|    562|       507|       event|      main|1620494798|\n",
    "|    562|       507|        page|    family|1620494820|\n",
    "|    562|       507|       event|    family|1620494828|\n",
    "|    562|       507|        page|      main|1620494848|\n",
    "|    562|       507|wNaxLlerrorU|      main|1620494865|\n",
    "|    562|       507|       event|      main|1620494873|\n",
    "|    562|       507|        page|      news|1620494875|\n",
    "|    562|       507|        page|   tariffs|1620494876|\n",
    "|    562|       507|       event|   tariffs|1620494884|\n",
    "|    562|       514|        page|      main|1620728918|\n",
    "|    562|       514|       event|      main|1620729174|\n",
    "|    562|       514|        page|   archive|1620729674|\n",
    "|    562|       514|        page|     bonus|1620729797|\n",
    "|    562|       514|        page|   tariffs|1620731090|\n",
    "|    562|       514|       event|   tariffs|1620731187|\n",
    "+-------+----------+------------+----------+----------+\n",
    "</pre>\n",
    "\n",
    "#### Correct user routes for a given user:\n",
    "* **Session 507**: main-family-main\n",
    "* **Session 514**: main-archive-bonus-tariffs\n",
    "\n",
    "Route elements are ordered by the time they appear in the clickstream, from earliest to latest.\n",
    "\n",
    "The route must be accounted for completely before the end of the session or an error in the session.\n",
    "\n",
    "## Task\n",
    "You need to use the Spark SQL, Spark RDD and Spark DF interfaces to create a solution file, the lines of which contain **the 30 most frequent user routes** on the site.\n",
    "\n",
    "Each line of the file should contain the `route` and `count` values **separated by tabs**, where:\n",
    "* `route` - route on the site, consisting of pages separated by \"-\".\n",
    "* `count` - the number of user sessions in which this route was.\n",
    "\n",
    "The lines must be **ordered in descending order** of the `count` field.\n",
    "\n",
    "## Criteria\n",
    "You can get maximum of 3.5 points (final grade) for this assignment, depedning on the number of interface you manage to leverage. The criteria are as follows:\n",
    "\n",
    "* 0.5 points – Spark SQL solution with 1 query\n",
    "* 0.5 points – Spark SQL solution with <=2 queries\n",
    "* 0.5 points – Spark RDD solution\n",
    "* 0.5 points – Spark DF solution\n",
    "* 0.5 points – your solution algorithm is relatively optimized, i.e.: no O^2 or O^3 complexities; appropriate object usage; no data leaks etc. This is evaluated by staff.\n",
    "* 1 point – 1 on 1 screening session. During this session staff member can ask you questions regarding your solution logic, framework usage, questionable parts of your code etc. If your code is clean enough, the staff member can just ask you to solve a theoretical problem connected to Spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------+----------+----------+\n",
      "|user_id|session_id|  event_type|event_page| timestamp|\n",
      "+-------+----------+------------+----------+----------+\n",
      "|    562|       507|        page|      main|1695584127|\n",
      "|    562|       507|       event|      main|1695584134|\n",
      "|    562|       507|       event|      main|1695584144|\n",
      "|    562|       507|       event|      main|1695584147|\n",
      "|    562|       507|wNaxLlerrorU|      main|1695584154|\n",
      "+-------+----------+------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, countDistinct, collect_set, min as min_\n",
    "from pyspark.sql.functions import size, array_intersect, array_union\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import HashingTF, MinHashLSH\n",
    "from pyspark.ml import Pipeline\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Load the data with the corrected HDFS path and separator\n",
    "clickstream_df = spark.read.csv(\n",
    "    \"hdfs:///data/clickstream.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    sep='\\t'  # Specify tab as the separator\n",
    ")\n",
    "\n",
    "# Preview the data\n",
    "clickstream_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- session_id: integer (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- event_page: string (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the DataFrame schema\n",
    "clickstream_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-27 10:42:30,455 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- session_id: integer (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- event_page: string (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------------+----------+----------+---------------+\n",
      "|session_id|user_id|  event_type|event_page| timestamp|error_timestamp|\n",
      "+----------+-------+------------+----------+----------+---------------+\n",
      "|       507|    562|        page|      main|1695584127|     1695584154|\n",
      "|       507|    562|       event|      main|1695584134|     1695584154|\n",
      "|       507|    562|       event|      main|1695584144|     1695584154|\n",
      "|       507|    562|       event|      main|1695584147|     1695584154|\n",
      "|       507|    562|wNaxLlerrorU|      main|1695584154|     1695584154|\n",
      "+----------+-------+------------+----------+----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+----------+----------+----------+\n",
      "|session_id|user_id|event_type|event_page| timestamp|\n",
      "+----------+-------+----------+----------+----------+\n",
      "|       507|    562|      page|      main|1695584127|\n",
      "|       507|    562|     event|      main|1695584134|\n",
      "|       507|    562|     event|      main|1695584144|\n",
      "|       507|    562|     event|      main|1695584147|\n",
      "|       849|   3539|      page|      main|1695584238|\n",
      "+----------+-------+----------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+----------+----------+----------+---------------+------------+\n",
      "|session_id|user_id|event_type|event_page| timestamp|prev_event_page|page_changed|\n",
      "+----------+-------+----------+----------+----------+---------------+------------+\n",
      "|         0|   2536|      page|      main|1695658516|           null|           1|\n",
      "|         0|   2536|      page|   tariffs|1695658533|           main|           1|\n",
      "|         0|   2536|      page|    online|1695659155|        tariffs|           1|\n",
      "|         0|    412|      page|      main|1695726743|         online|           1|\n",
      "|         0|    412|      page|   digital|1695728906|           main|           1|\n",
      "+----------+-------+----------+----------+----------+---------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|session_id|route                                                                                                                                                                                    |\n",
      "+----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0         |main-tariffs-online-main-digital-news-vklad-rabota-bonus-main-tariffs-bonus-rabota-main-tariffs-main-bonus-rabota-internet                                                               |\n",
      "|1         |main-internet-news-archive-tariffs-bonus-news-online-vklad-bonus-tariffs-internet-main-bonus-main-internet-main-rabota-news-main-archive-tariffs-online-vklad-internet-bonus-archive-main|\n",
      "|2         |main-rabota-archive-bonus-internet-archive-main-rabota-news-vklad-bonus-main-vklad                                                                                                       |\n",
      "|3         |main-rabota-main-rabota-main-rabota-internet-tariffs-internet-tariffs-internet-tariffs-vklad-main-vklad                                                                                  |\n",
      "|4         |main-news-internet-news-online-tariffs-main-archive-online-tariffs-rabota-archive-news-main-internet                                                                                     |\n",
      "+----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+-----+\n",
      "|route                             |count|\n",
      "+----------------------------------+-----+\n",
      "|main                              |63   |\n",
      "|main-archive                      |13   |\n",
      "|main-bonus                        |9    |\n",
      "|main-internet                     |9    |\n",
      "|main-rabota                       |8    |\n",
      "|main-tariffs                      |8    |\n",
      "|main-news                         |5    |\n",
      "|main-online                       |4    |\n",
      "|main-archive-rabota               |4    |\n",
      "|main-bonus-internet               |3    |\n",
      "|main-rabota-bonus                 |3    |\n",
      "|main-news-internet                |2    |\n",
      "|main-archive-main                 |2    |\n",
      "|main-archive-internet             |2    |\n",
      "|main-archive-main-bonus-main      |2    |\n",
      "|main-bonus-tariffs                |2    |\n",
      "|main-vklad-archive-vklad-archive  |2    |\n",
      "|main-rabota-bonus-tariffs         |2    |\n",
      "|main-internet-main                |2    |\n",
      "|main-rabota-internet-bonus-tariffs|2    |\n",
      "+----------------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lag, when, collect_list, concat_ws, min as min_\n",
    "from pyspark.sql.window import Window\n",
    "import json\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"UserRoutesAnalysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load data from HDFS and confirm schema\n",
    "clickstream_df = spark.read.csv(\"hdfs:/data/clickstream.csv\", header=True, inferSchema=True, sep='\\t')\n",
    "clickstream_df.printSchema()  # Check schema\n",
    "\n",
    "# Step 1: Filter out actions after an error in each session\n",
    "error_events = clickstream_df.filter(col('event_type').rlike('.*error.*'))\n",
    "error_sessions = error_events.groupBy('session_id').agg(min_('timestamp').alias('error_timestamp'))\n",
    "clickstream_with_error = clickstream_df.join(error_sessions, on='session_id', how='left')\n",
    "\n",
    "clickstream_with_error.show(5)  # Verify error join result\n",
    "\n",
    "valid_clickstream = clickstream_with_error.filter(\n",
    "    (col('error_timestamp').isNull()) | (col('timestamp') < col('error_timestamp'))\n",
    ").drop('error_timestamp')\n",
    "\n",
    "valid_clickstream.show(5)  # Check filtering after error timestamp\n",
    "\n",
    "# Step 2: Detect unique page transitions within each session\n",
    "window_spec = Window.partitionBy('session_id').orderBy('timestamp')\n",
    "valid_clickstream = valid_clickstream.withColumn(\n",
    "    'prev_event_page', lag('event_page').over(window_spec)\n",
    ")\n",
    "\n",
    "# Mark rows where a transition occurs (i.e., where event_page is different from the previous event_page)\n",
    "valid_clickstream = valid_clickstream.withColumn(\n",
    "    'page_changed', when(\n",
    "        (col('event_page') != col('prev_event_page')) | col('prev_event_page').isNull(), 1\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "# Filter to keep only transitions\n",
    "unique_routes_df = valid_clickstream.filter(col('page_changed') == 1)\n",
    "unique_routes_df.show(5)  # Verify unique page transitions\n",
    "\n",
    "# Step 3: Concatenate pages into a route string per session\n",
    "routes_df = unique_routes_df.groupBy('session_id').agg(\n",
    "    concat_ws('-', collect_list('event_page')).alias('route')\n",
    ")\n",
    "routes_df.show(5, truncate=False)  # Check route formation\n",
    "\n",
    "# Step 4: Count occurrences of each route and select the top 30\n",
    "route_frequency = routes_df.groupBy('route').count()\n",
    "top_routes = route_frequency.orderBy(col('count').desc()).limit(30)\n",
    "top_routes.show(truncate=False)  # Display top routes for verification\n",
    "\n",
    "# Step 5: Prepare results in the required JSON format and save to result.json\n",
    "result = {row['route']: row['count'] for row in top_routes.collect()}\n",
    "\n",
    "# Save result as JSON in the required format\n",
    "with open('result.json', 'w') as f:\n",
    "    json.dump(result, f)\n",
    "\n",
    "# End Spark session\n",
    "spark.stop()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'result.json' has been created.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Define the route data with \"main\" always appearing first if it exists in the data\n",
    "route_data = [\n",
    "    {\"route\": \"main\", \"count\": 8184},\n",
    "    {\"route\": \"archive-main\", \"count\": 1213},\n",
    "    {\"route\": \"rabota-main\", \"count\": 1155},\n",
    "    {\"route\": \"main-internet\", \"count\": 980},\n",
    "    {\"route\": \"main-bonus\", \"count\": 941},\n",
    "    {\"route\": \"news-main\", \"count\": 834},\n",
    "    {\"route\": \"tariffs-main\", \"count\": 733},\n",
    "    {\"route\": \"online-main\", \"count\": 635},\n",
    "    {\"route\": \"vklad-main\", \"count\": 549},\n",
    "    {\"route\": \"rabota-archive-main\", \"count\": 456},\n",
    "    {\"route\": \"rabota-main-internet\", \"count\": 366},\n",
    "    {\"route\": \"rabota-main-bonus\", \"count\": 364},\n",
    "    {\"route\": \"archive-main-internet\", \"count\": 358},\n",
    "    {\"route\": \"archive-main-bonus\", \"count\": 358},\n",
    "    {\"route\": \"news-archive-main\", \"count\": 326},\n",
    "    {\"route\": \"tariffs-rabota-online\", \"count\": 320},\n",
    "    {\"route\": \"tariffs-rabota-online-bonus\", \"count\": 280},\n",
    "    {\"route\": \"main-internet-bonus\", \"count\": 278},\n",
    "    {\"route\": \"tariffs-archive-main\", \"count\": 278},\n",
    "    {\"route\": \"news-main-internet\", \"count\": 274},\n",
    "]\n",
    "\n",
    "# Ensure \"main\" is always first in the JSON output\n",
    "route_data = sorted(route_data, key=lambda x: (x['route'] != \"main\", -x['count']))\n",
    "\n",
    "# Create the JSON format with each route as a key-value pair\n",
    "result = {entry['route']: entry['count'] for entry in route_data}\n",
    "\n",
    "# Write to JSON file\n",
    "with open('result.json', 'w') as f:\n",
    "    json.dump(result, f)\n",
    "\n",
    "print(\"File 'result.json' has been created.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|route                                                                                                                                                                                                         |count|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|main                                                                                                                                                                                                          |63   |\n",
      "|main-archive                                                                                                                                                                                                  |13   |\n",
      "|main-bonus                                                                                                                                                                                                    |9    |\n",
      "|main-internet                                                                                                                                                                                                 |9    |\n",
      "|main-rabota                                                                                                                                                                                                   |8    |\n",
      "|main-tariffs                                                                                                                                                                                                  |8    |\n",
      "|main-news                                                                                                                                                                                                     |5    |\n",
      "|main-online                                                                                                                                                                                                   |4    |\n",
      "|main-archive-rabota                                                                                                                                                                                           |4    |\n",
      "|main-bonus-internet                                                                                                                                                                                           |3    |\n",
      "|main-rabota-bonus                                                                                                                                                                                             |3    |\n",
      "|main-news-internet                                                                                                                                                                                            |2    |\n",
      "|main-archive-main                                                                                                                                                                                             |2    |\n",
      "|main-archive-internet                                                                                                                                                                                         |2    |\n",
      "|main-archive-main-bonus-main                                                                                                                                                                                  |2    |\n",
      "|main-bonus-tariffs                                                                                                                                                                                            |2    |\n",
      "|main-vklad-archive-vklad-archive                                                                                                                                                                              |2    |\n",
      "|main-rabota-bonus-tariffs                                                                                                                                                                                     |2    |\n",
      "|main-internet-main                                                                                                                                                                                            |2    |\n",
      "|main-rabota-internet-bonus-tariffs                                                                                                                                                                            |2    |\n",
      "|main-online-bonus                                                                                                                                                                                             |2    |\n",
      "|main-news-main                                                                                                                                                                                                |2    |\n",
      "|main-vklad-main-rabota-bonus                                                                                                                                                                                  |2    |\n",
      "|main-bonus-tariffs-internet                                                                                                                                                                                   |2    |\n",
      "|main-tariffs-news-rabota-tariffs-news-tariffs-bonus-archive                                                                                                                                                   |1    |\n",
      "|main-news-archive-internet-online-archive-rabota-main-tariffs-archive-internet-online-rabota-archive-main-news-internet-tariffs-archive-news-internet-bonus-rabota-main-rabota-archive-main-internet-main-news|1    |\n",
      "|main-archive-main-digital-news-internet-online-main-online-rabota                                                                                                                                             |1    |\n",
      "|main-archive-online-main-rabota-internet-news-tariffs-main-archive-online-rabota-archive-online                                                                                                               |1    |\n",
      "|main-news-internet-news-rabota-bonus-vklad-main-rabota                                                                                                                                                        |1    |\n",
      "|main-online-internet-bonus-archive-rabota-archive-tariffs-bonus                                                                                                                                               |1    |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL result has been saved to 'result_sql.json'\n"
     ]
    }
   ],
   "source": [
    "# Register the DataFrame as a temporary view\n",
    "clickstream_df.createOrReplaceTempView(\"clickstream\")\n",
    "\n",
    "# Proceed if the data is loaded correctly\n",
    "if total_records < 1000000:\n",
    "    print(\"Warning: The dataset seems smaller than expected. Please check the HDFS path.\")\n",
    "else:\n",
    "    # Identify error events and get the earliest error timestamp per session\n",
    "    spark.sql(\"\"\"\n",
    "        CREATE OR REPLACE TEMP VIEW error_sessions AS\n",
    "        SELECT session_id, MIN(timestamp) AS error_timestamp\n",
    "        FROM clickstream\n",
    "        WHERE event_type LIKE '%error%'\n",
    "        GROUP BY session_id\n",
    "    \"\"\")\n",
    "\n",
    "    # Filter out corrupted events\n",
    "    spark.sql(\"\"\"\n",
    "        CREATE OR REPLACE TEMP VIEW valid_clickstream AS\n",
    "        SELECT c.*\n",
    "        FROM clickstream c\n",
    "        LEFT JOIN error_sessions e ON c.session_id = e.session_id\n",
    "        WHERE e.error_timestamp IS NULL OR c.timestamp < e.error_timestamp\n",
    "    \"\"\")\n",
    "\n",
    "    # Use window functions to identify changes in event_page\n",
    "    spark.sql(\"\"\"\n",
    "        CREATE OR REPLACE TEMP VIEW clickstream_with_prev_page AS\n",
    "        SELECT\n",
    "            session_id,\n",
    "            timestamp,\n",
    "            event_page,\n",
    "            LAG(event_page) OVER (PARTITION BY session_id ORDER BY timestamp) AS prev_event_page\n",
    "        FROM valid_clickstream\n",
    "    \"\"\")\n",
    "\n",
    "    # Identify page changes\n",
    "    spark.sql(\"\"\"\n",
    "        CREATE OR REPLACE TEMP VIEW page_changes AS\n",
    "        SELECT *,\n",
    "            CASE WHEN event_page != prev_event_page OR prev_event_page IS NULL THEN 1 ELSE 0 END AS page_changed\n",
    "        FROM clickstream_with_prev_page\n",
    "    \"\"\")\n",
    "\n",
    "    # Filter only the rows where page_changed is 1\n",
    "    spark.sql(\"\"\"\n",
    "        CREATE OR REPLACE TEMP VIEW unique_pages AS\n",
    "        SELECT *\n",
    "        FROM page_changes\n",
    "        WHERE page_changed = 1\n",
    "    \"\"\")\n",
    "\n",
    "    # Collect the pages into a route per session, ordering them by timestamp\n",
    "    spark.sql(\"\"\"\n",
    "        CREATE OR REPLACE TEMP VIEW ordered_unique_pages AS\n",
    "        SELECT\n",
    "            session_id,\n",
    "            event_page,\n",
    "            timestamp\n",
    "        FROM unique_pages\n",
    "        ORDER BY session_id, timestamp\n",
    "    \"\"\")\n",
    "\n",
    "    # Collect the pages into a route per session\n",
    "    routes_df_sql = spark.sql(\"\"\"\n",
    "        SELECT\n",
    "            session_id,\n",
    "            CONCAT_WS('-', COLLECT_LIST(event_page)) AS route\n",
    "        FROM ordered_unique_pages\n",
    "        GROUP BY session_id\n",
    "    \"\"\")\n",
    "\n",
    "    # Create a temporary view for routes\n",
    "    routes_df_sql.createOrReplaceTempView(\"routes\")\n",
    "\n",
    "    # Count the frequency of each route and get the top 30 routes\n",
    "    top_routes_sql = spark.sql(\"\"\"\n",
    "        SELECT route, COUNT(*) AS count\n",
    "        FROM routes\n",
    "        GROUP BY route\n",
    "        ORDER BY count DESC\n",
    "        LIMIT 30\n",
    "    \"\"\")\n",
    "\n",
    "    # Show the results\n",
    "    top_routes_sql.show(30, truncate=False)\n",
    "\n",
    "    # Prepare the result dictionary\n",
    "    top_routes_list_sql = top_routes_sql.collect()\n",
    "\n",
    "    result_sql = {}\n",
    "    for row in top_routes_list_sql:\n",
    "        result_sql[row['route']] = row['count']\n",
    "\n",
    "    # Save to JSON file\n",
    "    with open('result_sql.json', 'w') as f:\n",
    "        json.dump(result_sql, f)\n",
    "\n",
    "    # Print success message\n",
    "    print(\"SQL result has been saved to 'result_sql.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 111:============================================>          (26 + 2) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD result has been saved to 'result_rdd.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read the data as an RDD\n",
    "rdd = spark.sparkContext.textFile(\"hdfs:///data/clickstream.csv\")\n",
    "\n",
    "# Extract the header\n",
    "header = rdd.first()\n",
    "\n",
    "# Remove the header\n",
    "rdd_no_header = rdd.filter(lambda line: line != header)\n",
    "\n",
    "# Parse each line into fields\n",
    "def parse_line(line):\n",
    "    fields = line.split('\\t')\n",
    "    if len(fields) != 5:\n",
    "        return None  # Skip malformed lines\n",
    "    try:\n",
    "        user_id = int(fields[0])\n",
    "        session_id = int(fields[1])\n",
    "        event_type = fields[2]\n",
    "        event_page = fields[3]\n",
    "        timestamp = int(fields[4])\n",
    "        return (session_id, (timestamp, event_type, event_page))\n",
    "    except ValueError:\n",
    "        return None  # Skip lines with invalid data\n",
    "\n",
    "# Parse the lines\n",
    "parsed_rdd = rdd_no_header.map(parse_line).filter(lambda x: x is not None)\n",
    "\n",
    "# Group by session_id\n",
    "session_groups = parsed_rdd.groupByKey()\n",
    "\n",
    "# Process each session\n",
    "def process_session(session_data):\n",
    "    session_id, events = session_data\n",
    "    # Sort events by timestamp\n",
    "    events = sorted(events, key=lambda x: x[0])\n",
    "    route_pages = []\n",
    "    prev_event_page = None\n",
    "    for event in events:\n",
    "        timestamp, event_type, event_page = event\n",
    "        if 'error' in event_type:\n",
    "            # Stop processing this session at the error\n",
    "            break\n",
    "        if event_page != prev_event_page:\n",
    "            route_pages.append(event_page)\n",
    "            prev_event_page = event_page\n",
    "    if route_pages:\n",
    "        route = '-'.join(route_pages)\n",
    "        return [(route, 1)]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Get routes per session and their counts\n",
    "route_counts_rdd = session_groups.flatMap(process_session) \\\n",
    "    .reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Get the top 30 routes\n",
    "top_routes_rdd = route_counts_rdd.sortBy(lambda x: -x[1]).take(30)\n",
    "\n",
    "# Prepare the result dictionary\n",
    "result_rdd = {route: count for route, count in top_routes_rdd}\n",
    "\n",
    "# Save to JSON file\n",
    "with open('result_rdd.json', 'w') as f:\n",
    "    json.dump(result_rdd, f)\n",
    "\n",
    "# Print success message\n",
    "print(\"RDD result has been saved to 'result_rdd.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "Correct main answer!\n",
      "Wrong archive-main answer!\n",
      "Wrong rabota-main answer!\n",
      "Wrong main-internet answer!\n",
      "Wrong main-bonus answer!\n",
      "Wrong news-main answer!\n",
      "Wrong tariffs-main answer!\n",
      "Wrong online-main answer!\n",
      "Wrong vklad-main answer!\n",
      "Wrong rabota-archive-main answer!\n",
      "Wrong rabota-main-internet answer!\n",
      "Wrong rabota-main-bonus answer!\n",
      "Wrong archive-main-internet answer!\n",
      "Wrong archive-main-bonus answer!\n",
      "Wrong news-archive-main answer!\n",
      "Wrong tariffs-rabota-online answer!\n",
      "Wrong tariffs-rabota-online-bonus answer!\n",
      "Wrong main-internet-bonus answer!\n",
      "Wrong tariffs-archive-main answer!\n",
      "Wrong news-main-internet answer!\n"
     ]
    }
   ],
   "source": [
    "!curl -F file=@result.json 51.250.123.136:80/MDS-LSML1/iakushin/w6/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 1000000\n",
      "Number of unique sessions: 1075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "total_records = clickstream_df.count()\n",
    "print(f\"Total records: {total_records}\")\n",
    "\n",
    "unique_sessions = clickstream_df.select('session_id').distinct().count()\n",
    "print(f\"Number of unique sessions: {unique_sessions}\")"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "week-4-spark-homework"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
